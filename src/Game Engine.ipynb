{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe713e1-f1e4-44f3-b6ea-9cc7a30187fb",
   "metadata": {},
   "source": [
    "# Arize Game Engine Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d9857-e797-4f35-8b68-8ae395fb0a23",
   "metadata": {},
   "source": [
    "### Load Passwords and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa922e-f50b-49c9-8673-ec9c83fac07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "for key in [\"OPENAI_API_KEY\"]:\n",
    "    if not os.environ.get(key):\n",
    "        print(f\"Please enter key: '{key}'\")\n",
    "        os.environ[key] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd96b3-7be5-41a1-a680-de607516d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import phoenix as px\n",
    "\n",
    "nest_asyncio.apply()\n",
    "px.close_app()\n",
    "px.launch_app()\n",
    "\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "LangChainInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681c5d5-b074-4795-9121-2014583d5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1df66-a60c-4db8-ba79-2c7903d64124",
   "metadata": {},
   "source": [
    "### Build the Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3611b84-c601-48c8-8244-81646251b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt = \"\"\"\n",
    "### General Instructions\n",
    "1. You are acting as the game engine for a text based adventure game. You are the equivalent of a D&D dungeon master.\n",
    "2. The human controlling the player character will issue instructions to move their character around. \n",
    "3. You will respond in a way consistent with their previous actions and with the game state. Never acknowledge that a game is being played.\n",
    "\n",
    "### Specific Instructions\n",
    "You must:\n",
    "1. Write in the second person and use the word \"you\" to describe the character\n",
    "2. Maintain inventory management\n",
    "3. Treat any text wrapped in square brackets as a hint as to where the player is. For example \"[Center Room] Look around\" should result in you describing the center room.\n",
    "\n",
    "### Inventory Rules\n",
    "1. When a player picks up an object, it is added to their inventory. It remains in their inventory until it is used. \n",
    "2. Inventory is empty to start \n",
    "3. Inventory has a capacity of 10 items\n",
    "\n",
    "### Cutscene Interactions\n",
    "1. When the player touches the glowing orb in the north room a cutscene should be triggered\n",
    "\n",
    "### Description Rules\n",
    "All descriptions come from room documents. You cannot make up any additional details.\n",
    "\n",
    "{context}\n",
    "\n",
    "User Interaction: {interaction}\n",
    "\n",
    "### Response:\n",
    "\n",
    "One to three sentence response to the user interaction.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e566874-0ff2-4264-831c-9984d90734ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Inventory(BaseModel):\n",
    "    items: List[str] = Field(..., description=\"items in the player's inventory\")\n",
    "    max_capacity: int = Field(10, description=\"the maximum number of items the player can hold\")\n",
    "\n",
    "class GameState(BaseModel):\n",
    "    inventory: Inventory = Field(..., description=\"the player's inventory\")\n",
    "    response: str = Field(..., description=\"your response to the player based on their latest interaction as described in the prompt. For example, the interaction 'what is in this room' should result in a description of the current room.\")\n",
    "    trigger_orb_cutscene: bool = Field(..., description=\"whether an orb cutscene should occur as described in the prompt\")\n",
    "    room: str = Field(\"Center Room\", description=\"the room in which the character is currently located as shown by the reference document. For example, if the reference document is titled 'Center Room' this should be filled with 'Center Room'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c779db-80f3-420d-864b-0f0aa2d8d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_retriever():\n",
    "    loader = DirectoryLoader('game_data', glob=\"room_*\", loader_cls=TextLoader)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "    return retriever\n",
    "\n",
    "def get_llm_chain():\n",
    "    prompt = ChatPromptTemplate.from_template(text_prompt)\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0).bind_tools([GameState])\n",
    "\n",
    "    retriever = load_retriever()\n",
    "    llm_chain = (\n",
    "        {\"context\": retriever | format_docs, \"interaction\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | JsonOutputToolsParser()\n",
    "    )\n",
    "    return llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d7d8c-410a-43e8-bbed-b3108de94314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GameEngine:\n",
    "    def __init__(self, llm_chain):\n",
    "        self.llm_chain = llm_chain\n",
    "        self.state = {}\n",
    "\n",
    "    def input(self, text):\n",
    "        new_state = self.llm_chain.invoke(text)\n",
    "        if len(new_state) == 0:\n",
    "            print(\"I'm sorry - I don't understand that command\")\n",
    "        else:\n",
    "            self.state = new_state[0]['args']\n",
    "\n",
    "    def react(self):\n",
    "        if self.state.get('response'):\n",
    "            print(self.state['response'])\n",
    "\n",
    "        if self.state.get('trigger_orb_cutscene'):\n",
    "            print(\"CUT SCENE TRIGGERED!\")\n",
    "\n",
    "def play_game():\n",
    "    game = GameEngine(get_llm_chain())\n",
    "\n",
    "    while True:\n",
    "        text = input(\">>> \")\n",
    "        if text == \"quit\":\n",
    "            break\n",
    "        game.input(text)\n",
    "        game.react()\n",
    "\n",
    "play_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f87de83-0ecc-4002-be22-aebb42c471ad",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "We care about quite a few different metrics for this text adventure game - tendency to hallucinate, document relevance, state tracking and whether key events get triggered at the right time.\n",
    "\n",
    "Let's focus on document relevance and tendency to hallucinate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b26af-9248-40b1-b4cf-4bccbd841f6b",
   "metadata": {},
   "source": [
    "#### Hallucination Evaluation Playground\n",
    "https://docs.arize.com/phoenix/evaluation/running-pre-tested-evals/hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639b96f-a003-4f70-b91f-097dcc4fc6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from phoenix.experimental.evals import (\n",
    "    HALLUCINATION_PROMPT_RAILS_MAP,\n",
    "    HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    download_benchmark_dataset,\n",
    "    llm_classify,\n",
    ")\n",
    "from pycm import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "llm_chain = get_llm_chain()\n",
    "loader = DirectoryLoader('game_data', glob=\"room_*\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "# Let's test some basic center room hallucinations\n",
    "hallucination_test_data = []\n",
    "for query in [\n",
    "    \"I pick up an orange\",\n",
    "    \"I take a candle from a candlestick\",\n",
    "    \"I look around and see President Barack Obama\"\n",
    "]:\n",
    "    game_state = llm_chain.invoke(query)\n",
    "    hallucination_test_data.append({\n",
    "        \"query\": query,\n",
    "        \"reference\": docs[0].page_content,\n",
    "        \"response\": game_state[0][\"args\"].get('response') if game_state else ''\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf43feb-40e4-4069-baf4-26a34bb26f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = (\n",
    "    pd.DataFrame(hallucination_test_data)\n",
    "    .rename(columns={'query': 'input', 'response': 'output'})\n",
    ")\n",
    "\n",
    "model = OpenAIModel(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "rails = list(HALLUCINATION_PROMPT_RAILS_MAP.values())\n",
    "hallucination_classifications = llm_classify(\n",
    "    dataframe=test_df, template=HALLUCINATION_PROMPT_TEMPLATE, model=model, rails=rails\n",
    ")\n",
    "\n",
    "hallucination_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a303e7a-7e02-41fc-a391-77f6bbea1b9d",
   "metadata": {},
   "source": [
    "#### Relevance Evaluation Playground\n",
    "Phoenix’s built-in RelevanceEvaluator doesn’t quite help us here. The default prompt doesn’t take into account the fact that we are using document retrieval in a pretty unorthodox manner and marks documents as “unrelated” even when they are related. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ce391-f994-44a3-a2c1-78cd64d8caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experimental.evals import RelevanceEvaluator\n",
    "from phoenix.experimental.evals import run_evals\n",
    "from phoenix.session.evaluation import get_retrieved_documents\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())\n",
    "\n",
    "eval_model = OpenAIModel(model=\"gpt-4-turbo-preview\", temperature=0.0)\n",
    "relevance_evaluator = RelevanceEvaluator(eval_model)\n",
    "relevance_eval_df = run_evals(\n",
    "    dataframe=retrieved_documents_df.tail(5),\n",
    "    evaluators=[relevance_evaluator],\n",
    "    provide_explanation=True,\n",
    ")[0]\n",
    "\n",
    "relevance_eval_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
